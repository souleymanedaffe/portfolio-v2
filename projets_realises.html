<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Projets Réalisés</title>
  <link rel="stylesheet" href="style.css">

  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VDNMV1EVS3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VDNMV1EVS3');
</script>
  
</head>
<body>
  <header>
    <h1>Projets Réalisés</h1>
    <nav>
       <a href="index.html">Accueil</a>
      <a href="competences.html">Compétences</a>
      <a href="experiences.html">Expériences</a>
      <a href="formation.html">Formation</a>
      <a href="projets_realises.html">Projets</a>
      <a href="certifications.html">Certifications</a>
      <a href="recommandations.html">Recommandations</a> |
      <a href="contact.html">Contact</a>
    </nav>
  </header>
  <main>

  <section class="project">
  <h2>Développement d'une application carbone</h2>
  <p>
    <strong>Contexte :</strong> Développement d’un outil innovant dans Revit pour estimer l’empreinte carbone des projets architecturaux dès les phases d’études préliminaires et d’avant-projet.  
    Cet outil permet de calculer une estimation globale du bilan carbone à partir des données disponibles dans le modèle Revit.  
    Il aide les décideurs à comprendre rapidement l’impact environnemental potentiel de leurs constructions et à prendre des décisions plus durables.
  </p>

  <p>
    L’outil a été conçu pour être facilement intégrable et simple d’utilisation. Il permet également de générer des rapports automatisés ainsi que des vues 3D colorées pour visualiser l’impact carbone de chaque composant du bâtiment.  
    j'ai  joué un rôle clé dans son développement, notamment dans la programmation en Python et l’intégration technique avec Revit.  
    Grâce à ma contribution, l’outil sera déployé chez AREP à l’été 2025.
  </p>

  <p><strong>Tâches effectuées :</strong> Développement d’une application sur Revit (Python/C#), génération automatique du calcul d’empreinte carbone, visualisation interactive avec Dash & Power BI, assistance à la prise de décision écologique.</p>

  <p><strong>Outils :</strong> Python, Revit, C#, Dash, Power BI, Git, GitHub.</p>

</section>

  

 <section class="project">
  <h2>Prévision de la consommation d’énergie électrique</h2>
  <p><strong>Contexte :</strong> Analyse de séries temporelles visant à prévoir la consommation électrique et à détecter des anomalies ou des effets saisonniers. L’objectif principal est :</p>
  <ul>
    <li>de comprendre l’évolution de la consommation d’électricité au fil du temps,</li>
    <li>de détecter des comportements saisonniers (par exemple, une hausse en hiver),</li>
    <li>d’identifier des anomalies ou des pics inhabituels,</li>
    <li>et surtout, de prévoir la consommation future pour mieux planifier la production.</li>
  </ul>

  <p><strong>Tâches effectuées :</strong> 
  J’ai analysé une série temporelle de 2012 à 2021 afin de :
  <ul>
    <li>mettre en évidence les variables les plus influentes sur la consommation,</li>
    <li>détecter une éventuelle saisonnalité,</li>
    <li>préparer les données pour appliquer un modèle ARIMA ou SARIMA.</li>
  </ul>
  J’ai utilisé la méthode SARIMA pour réaliser les prédictions, en automatisant la sélection des meilleurs paramètres grâce à une fonction Python dédiée. Le modèle final a été évalué à l’aide de visualisations et d’indicateurs statistiques.
  </p>

  <p><strong>Outils :</strong> Python, ARIMA, SARIMA, SARIMAX, matplotlib, seaborn, statsmodels</p>
</section>



    <section class="project">
  <h2>Détection de fake news via Deep Learning</h2>
  <p><strong>Contexte :</strong> Lutte contre la désinformation diffusée sur les réseaux sociaux, notamment concernant la santé des dirigeants.</p>

  <p><strong>Tâches effectuées :</strong> Collecte de données (Facebook, Twitter, etc.), traitement en CSV, NLP, entraînement de modèles pour la détection automatique de fausses nouvelles.</p>

  <p>Pour ce projet, j’ai choisi de m’appuyer sur l’intelligence artificielle (IA), qui offre une solution prometteuse pour automatiser la détection des fake news. En utilisant des techniques avancées de traitement du langage naturel (NLP), j’ai récolté des données à partir de diverses sources telles que Facebook, Google, Twitter, la presse et YouTube.</p>

  <p>Par exemple, j’ai récupéré toutes les vidéos évoquant les fausses nouvelles sur la santé du président. Je me suis concentré sur les commentaires trouvés sur Google, j’ai collecté des tweets liés à ce sujet sur Twitter, rassemblé des articles de presse, et analysé les publications et commentaires sur Facebook.</p>

  <p>J’ai ensuite structuré mes données dans un fichier CSV comportant quatre colonnes :
    <ul>
      <li><strong>sources :</strong> origine des données (site web ou plateforme sociale),</li>
      <li><strong>commentaire :</strong> texte de la nouvelle ou du commentaire,</li>
      <li><strong>malade :</strong> identification du sujet (ici, la santé du président),</li>
      <li><strong>auteur :</strong> nom ou identifiant de l’auteur du contenu.</li>
    </ul>
  </p>

  <p>Après avoir préparé ces fichiers CSV, j’ai utilisé la bibliothèque Pandas en Python pour les concaténer en un seul fichier. Grâce à la fonction <code>concat()</code>, j’ai pu fusionner toutes les données en un fichier consolidé prêt à l’analyse.</p>

  <p>Ce fichier unique contient l’ensemble des données structurées. La colonne "sources" permet de retracer l’origine de chaque entrée, "commentaire" fournit le contenu textuel, "malade" permet d’identifier les fausses nouvelles liées à la santé, et "auteur" indique l’origine du contenu.</p>

  <p>Ce processus de collecte et de structuration des données est essentiel pour entraîner un système d’IA performant. Il facilite l’analyse des tendances, la détection des motifs récurrents dans les fausses nouvelles, et l’identification de sources peu fiables.</p>

  <p>En résumé, j’ai pris en charge l’intégralité de la collecte et de l’organisation des données, créant ainsi une base robuste pour mon projet de détection de fake news.</p>

  <p><strong>Outils :</strong> Python, Pandas, NumPy, Scikit-learn, TensorFlow, Matplotlib, Seaborn, CountVectorizer, TfidfVectorizer, Stopwords, Logistic Regression, Random Forest, SVM, <code>accuracy_score</code>, <code>f1_score</code>, <code>confusion_matrix</code>.</p>

</section>




  <section class="project">
  <h2>Assistant IA – Jeux Olympiques 2024</h2>

  <p><strong>Contexte :</strong>  
  Développement d’un chatbot prédictif basé sur l’IA pour répondre à toutes les questions liées aux Jeux Olympiques de Paris 2024, notamment les classements, les performances des athlètes, et les prévisions de médailles.</p>

  <p><strong>Tâches effectuées :</strong>  
  J’ai conçu et testé plusieurs prompts spécialisés afin d’adapter les réponses du chatbot selon le contexte :  
  <ul>
    <li><code>JO_PROMPT</code> : Fournit les informations officielles sur le classement des JO 2024.</li>
    <li><code>JO_PROMPT2</code> : Présente les performances remarquables et les médaillés marquants des JO.</li>
    <li><code>JO_PROMPT3</code> : Prompt destiné à un assistant expert en sport. Il analyse les chances de médailles d’un pays ou d’un athlète en se basant sur les résultats précédents, les classements mondiaux, et les tendances observées, sans jamais donner de certitudes.</li>
  </ul>
  Ce système de prompts permet une réponse contextuelle, fiable et nuancée selon le type de requête utilisateur.</p>

  <p><strong>Outils :</strong> Python, OpenAI API, Streamlit</p>
</section>


  <section class="project">
  <h2>Extraction de données depuis des documents PDF</h2>

  <p><strong>Contexte :</strong>  
  L’objectif était de collecter des données à partir de documents PDF (avec ou sans couche texte), d’extraire automatiquement des informations clés et de les structurer dans un fichier CSV contenant les colonnes suivantes :  
  <code>"enterprise_name", "raison_d_etre", "objectifs_sociaux", "objectifs_environnementaux", "mission_nature"</code>.  
  Le projet s’appuie sur des techniques d’OCR, de traitement de texte et d’intelligence artificielle (modèle OpenAI GPT).</p>

  <p><strong>Tâches effectuées :</strong>  
    - Identification des PDF contenant une couche texte ou non à l’aide de <code>pdfplumber</code> ;<br>
    - Utilisation de <code>pdfplumber</code> pour les PDF textuels, et de <code>pytesseract</code> pour l’OCR des PDF scannés ;<br>
    - Développement d’une fonction de découpage du texte en fragments pour respecter les limites de tokens des modèles IA ;<br>
    - Création d’un système de requêtes structurées vers le modèle GPT pour extraire des informations spécifiques (nom d’entreprise, raison d’être, missions RSE, etc.) ;<br>
    - Implémentation d’une logique d’agrégation des résultats issus de plusieurs pages (chunks) ;<br>
    - Traitement automatisé d’un dossier complet de PDF, avec sauvegarde des résultats dans un fichier CSV ;<br>
    - Gestion des erreurs, vérification des réponses de l’API, gestion des cas d’échec avec fallback JSON sécurisé.</p>

  <p><strong>Outils :</strong> Python, pdfplumber, pytesseract, OpenAI API, CSV, JSON</p>
</section>

    
    <section class="project">
    <h2>Calcul de la performance commerciale, analyse de données et visualisation avec Power BI</h2>
    <p><strong>Contexte :</strong> Création d’un tableau de bord Power BI mettant en avant :
      <ul>
        <li>Les produits les plus performants</li>
        <li>Les pays ou clients les plus rentables</li>
        <li>Des indicateurs de succès clairs</li>
        <li>Des axes d'amélioration potentiels</li>
      </ul>
    </p>
    <p><strong>Tâches effectuées :</strong> Pour évaluer la performance, j’ai d’abord mis en place une table de dates. Ensuite, j’ai réalisé plusieurs opérations : création d’une colonne pour le montant total des ventes, définition d’une mesure pour le total des ventes, puis calcul des ventes des années précédentes afin de mesurer les performances des produits. J’ai également créé une mesure nommée "Ventes précédentes", permettant cette comparaison, ainsi qu’une mesure "Évolution des ventes" pour visualiser les tendances par produit.</p>
    <p><strong>Outils :</strong> Power BI, DAX, Power Query, mesures, colonnes calculées, Excel</p>
  </section>

    <section class="project">
  <h2>Scraping de données web et traitement analytique</h2>
  
  <p><strong>Contexte :</strong> 
    Ce projet s’inscrit dans une démarche d’apprentissage du traitement et de l’analyse de données.  
    Il consistait à extraire des informations librement accessibles sur Internet, notamment depuis Wikipedia.  
    Le sujet portait sur une liste de pays avec des indicateurs comme la population et la superficie, disponibles sous forme de tableaux HTML.  
    L’objectif était de transformer ces données brutes en un jeu de données structuré, exploitable pour l’analyse et la visualisation.  
    Ce projet a permis de développer des compétences en web scraping, manipulation de données et visualisation avec Python.
  </p>
  
  <p><strong>Tâches effectuées :</strong> 
    J’ai identifié une page Wikipedia contenant des données sur les pays (population, superficie, etc.).  
    À l’aide de l’inspecteur HTML, j’ai localisé les balises <code>&lt;table&gt;</code> pertinentes, puis utilisé la bibliothèque <code>requests</code> pour récupérer le contenu.  
    Ensuite, j’ai extrait les données ciblées à l’aide de <code>BeautifulSoup</code>, nettoyé les formats, supprimé les balises HTML inutiles, et converti les types de données.  
    Les informations ont été structurées dans un <code>DataFrame</code> avec <code>pandas</code>, analysées selon différents critères, puis visualisées via des graphiques.  
    Enfin, le jeu de données a été exporté au format CSV.
  </p>

  <p><strong>Outils :</strong> Python, requests, BeautifulSoup, pandas, matplotlib, seaborn (utilisés dans un Jupyter Notebook).</p>
</section>

  </main>
  <footer>
    <p>Souleymane Daffe  DATA SCIENTIS/ANALYST/DEV IA</p>
  </footer>
</body>
</html>
