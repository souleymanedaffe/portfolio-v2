<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Projets Réalisés</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>Projets Réalisés</h1>
    <nav>
      <a href="index.html">Accueil</a>
      <a href="competences.html">Compétences</a>
      <a href="experiences.html">Expériences</a>
      <a href="formation.html">Formation</a>
      <a href="contact.html">Contact</a>
      <a href="projets_realises.html">Projets</a>
      <a href="certifications.html">Certifications</a>
    </nav>
  </header>
  <main>
    <section class="project">
      <h2>Développement d'application carbone</h2>
      <p><strong>Contexte :</strong> Outil Revit pour estimer l’empreinte carbone de projets architecturaux dès les phases préliminaires.</p>
      <p><strong>Tâches effectuées :</strong> Développement d'une app sur Revit (Python/C#), génération automatique de l'empreinte carbone, visualisation avec Dash & PowerBI, aide à la décision.</p>
      <p><strong>Outils :</strong> Python, Revit, C#, Dash, PowerBI</p>
    </section>

    <section class="project">
      <h2>Prévision de la consommation d’énergie électrique</h2>
      <p><strong>Contexte :</strong> Analyse de séries temporelles pour prévoir la consommation électrique et détecter anomalies/saisonnalité.</p>
      <p><strong>Tâches effectuées :</strong> Analyse SARIMA/SARIMAX de données (2012-2021), sélection automatique des meilleurs paramètres, prédictions.</p>
      <p><strong>Outils :</strong> Python, ARIMA, SARIMA, SARIMAX, matplotlib, seaborn, statsmodels</p>
    </section>


    <section class="project">
  <h2>Détection de fake news via Deep Learning</h2>
  <p><strong>Contexte :</strong> Lutte contre la désinformation diffusée sur les réseaux sociaux, notamment concernant la santé des dirigeants.</p>

  <p><strong>Tâches effectuées :</strong> Collecte de données (Facebook, Twitter, etc.), traitement en CSV, NLP, entraînement de modèles pour la détection automatique de fausses nouvelles.</p>

  <p>Pour ce projet, j’ai choisi de m’appuyer sur l’intelligence artificielle (IA), qui offre une solution prometteuse pour automatiser la détection des fake news. En utilisant des techniques avancées de traitement du langage naturel (NLP), j’ai récolté des données à partir de diverses sources telles que Facebook, Google, Twitter, la presse et YouTube.</p>

  <p>Par exemple, j’ai récupéré toutes les vidéos évoquant les fausses nouvelles sur la santé du président. Je me suis concentré sur les commentaires trouvés sur Google, j’ai collecté des tweets liés à ce sujet sur Twitter, rassemblé des articles de presse, et analysé les publications et commentaires sur Facebook.</p>

  <p>J’ai ensuite structuré mes données dans un fichier CSV comportant quatre colonnes :
    <ul>
      <li><strong>sources :</strong> origine des données (site web ou plateforme sociale),</li>
      <li><strong>commentaire :</strong> texte de la nouvelle ou du commentaire,</li>
      <li><strong>malade :</strong> identification du sujet (ici, la santé du président),</li>
      <li><strong>auteur :</strong> nom ou identifiant de l’auteur du contenu.</li>
    </ul>
  </p>

  <p>Après avoir préparé ces fichiers CSV, j’ai utilisé la bibliothèque Pandas en Python pour les concaténer en un seul fichier. Grâce à la fonction <code>concat()</code>, j’ai pu fusionner toutes les données en un fichier consolidé prêt à l’analyse.</p>

  <p>Ce fichier unique contient l’ensemble des données structurées. La colonne "sources" permet de retracer l’origine de chaque entrée, "commentaire" fournit le contenu textuel, "malade" permet d’identifier les fausses nouvelles liées à la santé, et "auteur" indique l’origine du contenu.</p>

  <p>Ce processus de collecte et de structuration des données est essentiel pour entraîner un système d’IA performant. Il facilite l’analyse des tendances, la détection des motifs récurrents dans les fausses nouvelles, et l’identification de sources peu fiables.</p>

  <p>En résumé, j’ai pris en charge l’intégralité de la collecte et de l’organisation des données, créant ainsi une base robuste pour mon projet de détection de fake news.</p>

  <p><strong>Outils :</strong> Python, stopwords, Logistic Regression, Random Forest, Matplotlib, Seaborn</p>
</section>




    <section class="project">
      <h2>Assistant IA Jeux Olympiques 2024</h2>
      <p><strong>Contexte :</strong> Chatbot prédictif basé sur prompts IA pour répondre à toutes les questions sportives et JO 2024.</p>
      <p><strong>Tâches effectuées :</strong> Rédaction de prompts spécialisés, appel d'API OpenAI, interface avec Streamlit.</p>
      <p><strong>Outils :</strong> Python, OpenAI API, Streamlit</p>
    </section>

    <section class="project">
      <h2>Extraction de données depuis des documents PDF</h2>
      <p><strong>Contexte :</strong> Analyse automatique de documents PDF (textes + OCR), extraction et structuration de données RSE.</p>
      <p><strong>Tâches effectuées :</strong> OCR avec pytesseract, extraction avec pdfplumber, découpage pour GPT, agrégation de réponses, génération CSV.</p>
      <p><strong>Outils :</strong> Python, pdfplumber, pytesseract, OpenAI API, CSV, json</p>
    </section>
    
    <section class="project">
    <h2>Calcul de la performance commerciale, analyse de données et visualisation avec Power BI</h2>
    <p><strong>Contexte :</strong> Création d’un tableau de bord Power BI mettant en avant :
      <ul>
        <li>Les produits les plus performants</li>
        <li>Les pays ou clients les plus rentables</li>
        <li>Des indicateurs de succès clairs</li>
        <li>Des axes d'amélioration potentiels</li>
      </ul>
    </p>
    <p><strong>Tâches effectuées :</strong> Pour évaluer la performance, j’ai d’abord mis en place une table de dates. Ensuite, j’ai réalisé plusieurs opérations : création d’une colonne pour le montant total des ventes, définition d’une mesure pour le total des ventes, puis calcul des ventes des années précédentes afin de mesurer les performances des produits. J’ai également créé une mesure nommée "Ventes précédentes", permettant cette comparaison, ainsi qu’une mesure "Évolution des ventes" pour visualiser les tendances par produit.</p>
    <p><strong>Outils :</strong> Power BI, DAX, Power Query, mesures, colonnes calculées, Excel</p>
  </section>

  </main>
  <footer>
    <p>Souleymane Daffe  DATA SCIENTIS/ANALYST/DEV IA</p>
  </footer>
</body>
</html>
