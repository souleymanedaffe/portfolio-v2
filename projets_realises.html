<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Projets R√©alis√©s</title>
  <link rel="stylesheet" href="style.css">

  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VDNMV1EVS3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VDNMV1EVS3');
</script>
  
</head>
<body>
  <header>
    <h1>Projets R√©alis√©s</h1>
    <nav>
       <a href="index.html">Accueil</a>
      <a href="competences.html">Comp√©tences</a>
      <a href="experiences.html">Exp√©riences</a>
      <a href="formation.html">Formation</a>
      <a href="projets_realises.html">Projets</a>
      <a href="certifications.html">Certifications</a>
       <a href="recommandations.html">Recommandations</a> |
      <a href="contact.html">Contact</a>
    </nav>
  </header>
  <main>

 <h1>Projets R√©alis√©s en Entreprise et en Milieu Acad√©mique</h1>
    <h2>Projets Acad√©miques</h2>
<p>
  üîó Lien vers mon projet acad√©mique : 
  <a href="https://github.com/souleymanedaffe/JO" target="_blank">
    https://github.com/souleymanedaffe/JO
  </a>
</p>


 <section class="project">
  <h2>Pr√©vision de la consommation d‚Äô√©nergie √©lectrique</h2>
  <p><strong>Contexte :</strong> Analyse de s√©ries temporelles visant √† pr√©voir la consommation √©lectrique et √† d√©tecter des anomalies ou des effets saisonniers. L‚Äôobjectif principal est :</p>
  <ul>
    <li>de comprendre l‚Äô√©volution de la consommation d‚Äô√©lectricit√© au fil du temps,</li>
    <li>de d√©tecter des comportements saisonniers (par exemple, une hausse en hiver),</li>
    <li>d‚Äôidentifier des anomalies ou des pics inhabituels,</li>
    <li>et surtout, de pr√©voir la consommation future pour mieux planifier la production.</li>
  </ul>

  <p><strong>T√¢ches effectu√©es :</strong> 
  J‚Äôai analys√© une s√©rie temporelle de 2012 √† 2021 afin de :
  <ul>
    <li>mettre en √©vidence les variables les plus influentes sur la consommation,</li>
    <li>d√©tecter une √©ventuelle saisonnalit√©,</li>
    <li>pr√©parer les donn√©es pour appliquer un mod√®le ARIMA ou SARIMA.</li>
  </ul>
  J‚Äôai utilis√© la m√©thode SARIMA pour r√©aliser les pr√©dictions, en automatisant la s√©lection des meilleurs param√®tres gr√¢ce √† une fonction Python d√©di√©e. Le mod√®le final a √©t√© √©valu√© √† l‚Äôaide de visualisations et d‚Äôindicateurs statistiques.
  </p>

  <p><strong>Outils :</strong> Python, ARIMA, SARIMA, SARIMAX, matplotlib, seaborn, statsmodels</p>
</section>

 <section class="project">
  <h2>Pr√©diction du Prix des Appartements avec le Machine Learning</h2>

  <p><strong>Contexte :</strong><br>
    Le march√© immobilier est influenc√© par de nombreux facteurs tels que la localisation, la surface habitable, le nombre de pi√®ces ou encore l'ann√©e de construction.  
    Dans ce projet, j‚Äôai cherch√© √† cr√©er un mod√®le pr√©dictif fiable du prix de vente d‚Äôun appartement en me basant sur ses caract√©ristiques principales.  
    L‚Äôobjectif est d‚Äôaider les agences, les particuliers ou les investisseurs √† estimer plus justement la valeur d‚Äôun bien.
  </p>

  <p><strong>T√¢ches effectu√©es :</strong></p>
  <ol>
    <li><strong>Collecte de donn√©es</strong><br>
      J‚Äôai utilis√© un dataset public provenant de la plateforme Kaggle. Il contient des informations d√©taill√©es sur des appartements : surface, nombre de pi√®ces, localisation, ann√©e de construction, ainsi que le prix de vente.  
      J‚Äôaurais √©galement pu recourir au web scraping (sur des sites comme LeBonCoin ou SeLoger), mais j‚Äôai pr√©f√©r√© une base d√©j√† structur√©e pour me concentrer sur l‚Äôanalyse et la mod√©lisation.
    </li>

    <li><strong>Exploration et Pr√©traitement des Donn√©es</strong>
      <ul>
        <li><strong>Analyse exploratoire (EDA)</strong> : j‚Äôai utilis√© des statistiques descriptives et des visualisations (histogrammes, boxplots, heatmap de corr√©lation) pour mieux comprendre la distribution des variables et identifier les relations avec le prix.</li>
        <li><strong>Nettoyage des donn√©es</strong> : j‚Äôai supprim√© ou imput√© les valeurs manquantes, √©limin√© les doublons et v√©rifi√© la coh√©rence des donn√©es.</li>
        <li><strong>Transformation des variables</strong> :
          <ul>
            <li>J‚Äôai encod√© les variables cat√©gorielles (comme la localisation) avec <code>OneHotEncoder</code>.</li>
            <li>J‚Äôai standardis√© et normalis√© les variables num√©riques pour les mod√®les sensibles √† l‚Äô√©chelle (comme KNN).</li>
            <li>J‚Äôai √©galement cr√©√© de nouvelles variables comme le prix au m¬≤ ou l‚Äô√¢ge du b√¢timent.</li>
          </ul>
        </li>
      </ul>
    </li>

    <li><strong>Mod√©lisation avec le Machine Learning</strong>
      <ul>
        <li>J‚Äôai divis√© le dataset en deux parties : 80‚ÄØ% pour l'entra√Ænement et 20‚ÄØ% pour les tests.</li>
        <li>J‚Äôai entra√Æn√© plusieurs mod√®les de r√©gression afin de comparer leurs performances :
          <ul>
            <li>R√©gression Lin√©aire</li>
            <li>K-Nearest Neighbors (KNN)</li>
            <li>Random Forest Regressor</li>
            <li>Gradient Boosting (XGBoost et LightGBM)</li>
          </ul>
        </li>
        <li>Pour les √©valuer, j‚Äôai utilis√© les m√©triques suivantes :
          <ul>
            <li>MAE (Mean Absolute Error)</li>
            <li>RMSE (Root Mean Squared Error)</li>
            <li>R¬≤ (coefficient de d√©termination)</li>
            <li>Et des graphes de pr√©diction vs valeurs r√©elles</li>
          </ul>
        </li>
      </ul>
    </li>

    <li><strong>Optimisation des mod√®les</strong>
      <ul>
        <li>J‚Äôai optimis√© les hyperparam√®tres gr√¢ce √† <code>GridSearchCV</code> et <code>RandomizedSearchCV</code>.</li>
        <li>J‚Äôai effectu√© une s√©lection des variables les plus importantes pour simplifier les mod√®les et √©viter l‚Äôoverfitting.</li>
      </ul>
    </li>

    <li><strong>Visualisation et interpr√©tation</strong>
      <ul>
        <li>J‚Äôai affich√© une heatmap de corr√©lation pour visualiser les d√©pendances entre variables.</li>
        <li>J‚Äôai analys√© l‚Äôimportance des variables dans les mod√®les bas√©s sur les arbres (Random Forest, XGBoost).</li>
        <li>J‚Äôai utilis√© des courbes d‚Äôapprentissage pour d√©tecter d‚Äô√©ventuels probl√®mes de sous-apprentissage ou sur-apprentissage.</li>
      </ul>
    </li>
  </ol>

  <p><strong>Outils & Biblioth√®ques utilis√©s :</strong></p>
  <ul>
    <li><strong>Langage :</strong> Python 3.x</li>
    <li><strong>Analyse de donn√©es :</strong>
      <ul>
        <li><code>pandas</code></li>
        <li><code>numpy</code></li>
      </ul>
    </li>
    <li><strong>Visualisation :</strong>
      <ul>
        <li><code>matplotlib</code></li>
        <li><code>seaborn</code></li>
        <li><code>plotly</code> (pour les visualisations interactives)</li>
      </ul>
    </li>
    <li><strong>Machine Learning :</strong>
      <ul>
        <li><code>scikit-learn</code> : r√©gression, KNN, Random Forest, m√©triques d‚Äô√©valuation</li>
        <li><code>xgboost</code> et <code>lightgbm</code> : Gradient Boosting</li>
      </ul>
    </li>
    <li><strong>Optimisation :</strong>
      <ul>
        <li><code>sklearn.model_selection</code> : <code>train_test_split</code>, <code>GridSearchCV</code>, <code>RandomizedSearchCV</code></li>
      </ul>
    </li>
  </ul>
</section>


  
    <section class="project">
    <h2>Calcul de la performance commerciale, analyse de donn√©es et visualisation avec Power BI</h2>
    <p><strong>Contexte :</strong> Cr√©ation d‚Äôun tableau de bord Power BI mettant en avant :
      <ul>
        <li>Les produits les plus performants</li>
        <li>Les pays ou clients les plus rentables</li>
        <li>Des indicateurs de succ√®s clairs</li>
        <li>Des axes d'am√©lioration potentiels</li>
      </ul>
    </p>
    <p><strong>T√¢ches effectu√©es :</strong> Pour √©valuer la performance, j‚Äôai d‚Äôabord mis en place une table de dates. Ensuite, j‚Äôai r√©alis√© plusieurs op√©rations : cr√©ation d‚Äôune colonne pour le montant total des ventes, d√©finition d‚Äôune mesure pour le total des ventes, puis calcul des ventes des ann√©es pr√©c√©dentes afin de mesurer les performances des produits. J‚Äôai √©galement cr√©√© une mesure nomm√©e "Ventes pr√©c√©dentes", permettant cette comparaison, ainsi qu‚Äôune mesure "√âvolution des ventes" pour visualiser les tendances par produit.</p>
    <p><strong>Outils :</strong> Power BI, DAX, Power Query, mesures, colonnes calcul√©es, Excel</p>
  </section>

    <section class="project">
  <h2>Scraping de donn√©es web et traitement analytique</h2>
  
  <p><strong>Contexte :</strong> 
    Ce projet s‚Äôinscrit dans une d√©marche d‚Äôapprentissage du traitement et de l‚Äôanalyse de donn√©es.  
    Il consistait √† extraire des informations librement accessibles sur Internet, notamment depuis Wikipedia.  
    Le sujet portait sur une liste de pays avec des indicateurs comme la population et la superficie, disponibles sous forme de tableaux HTML.  
    L‚Äôobjectif √©tait de transformer ces donn√©es brutes en un jeu de donn√©es structur√©, exploitable pour l‚Äôanalyse et la visualisation.  
    Ce projet a permis de d√©velopper des comp√©tences en web scraping, manipulation de donn√©es et visualisation avec Python.
  </p>
  
  <p><strong>T√¢ches effectu√©es :</strong> 
    J‚Äôai identifi√© une page Wikipedia contenant des donn√©es sur les pays (population, superficie, etc.).  
    √Ä l‚Äôaide de l‚Äôinspecteur HTML, j‚Äôai localis√© les balises <code>&lt;table&gt;</code> pertinentes, puis utilis√© la biblioth√®que <code>requests</code> pour r√©cup√©rer le contenu.  
    Ensuite, j‚Äôai extrait les donn√©es cibl√©es √† l‚Äôaide de <code>BeautifulSoup</code>, nettoy√© les formats, supprim√© les balises HTML inutiles, et converti les types de donn√©es.  
    Les informations ont √©t√© structur√©es dans un <code>DataFrame</code> avec <code>pandas</code>, analys√©es selon diff√©rents crit√®res, puis visualis√©es via des graphiques.  
    Enfin, le jeu de donn√©es a √©t√© export√© au format CSV.
  </p>

  <p><strong>Outils :</strong> Python, requests, BeautifulSoup, pandas, matplotlib, seaborn (utilis√©s dans un Jupyter Notebook).</p>
</section>

     <section class="project">
  <h2>Extraction de donn√©es depuis des documents PDF</h2>

  <p><strong>Contexte :</strong>  
  L‚Äôobjectif √©tait de collecter des donn√©es √† partir de documents PDF (avec ou sans couche texte), d‚Äôextraire automatiquement des informations cl√©s et de les structurer dans un fichier CSV contenant les colonnes suivantes :  
  <code>"enterprise_name", "raison_d_etre", "objectifs_sociaux", "objectifs_environnementaux", "mission_nature"</code>.  
  Le projet s‚Äôappuie sur des techniques d‚ÄôOCR, de traitement de texte et d‚Äôintelligence artificielle (mod√®le OpenAI GPT).</p>

  <p><strong>T√¢ches effectu√©es :</strong>  
    - Identification des PDF contenant une couche texte ou non √† l‚Äôaide de <code>pdfplumber</code> ;<br>
    - Utilisation de <code>pdfplumber</code> pour les PDF textuels, et de <code>pytesseract</code> pour l‚ÄôOCR des PDF scann√©s ;<br>
    - D√©veloppement d‚Äôune fonction de d√©coupage du texte en fragments pour respecter les limites de tokens des mod√®les IA ;<br>
    - Cr√©ation d‚Äôun syst√®me de requ√™tes structur√©es vers le mod√®le GPT pour extraire des informations sp√©cifiques (nom d‚Äôentreprise, raison d‚Äô√™tre, missions RSE, etc.) ;<br>
    - Impl√©mentation d‚Äôune logique d‚Äôagr√©gation des r√©sultats issus de plusieurs pages (chunks) ;<br>
    - Traitement automatis√© d‚Äôun dossier complet de PDF, avec sauvegarde des r√©sultats dans un fichier CSV ;<br>
    - Gestion des erreurs, v√©rification des r√©ponses de l‚ÄôAPI, gestion des cas d‚Äô√©chec avec fallback JSON s√©curis√©.</p>

  <p><strong>Outils :</strong> Python, pdfplumber, pytesseract, OpenAI API, CSV, JSON</p>
</section>

  <section class="project">
  <h2>Analyse des Risques d'Ob√©sit√© dans les Populations du Mexique, du P√©rou et de la Colombie : Application de l'Intelligence Artificielle aux Habitudes Alimentaires et √† la Condition Physique</h2>
  
  <p><strong>Contexte :</strong><br>
    L‚Äôob√©sit√© est un enjeu de sant√© publique croissant en Am√©rique latine, notamment au Mexique, au P√©rou et en Colombie. Ce projet vise √† analyser les facteurs de risque (alimentation, activit√© physique, habitudes de vie) li√©s √† l‚Äôob√©sit√© dans ces pays, en utilisant des techniques d‚Äôintelligence artificielle.<br>
    Les donn√©es collect√©es concernent 2111 individus avec 18 variables (9 qualitatives, 8 quantitatives et 1 calcul√©e : l‚ÄôIMC). La variable cible est <em>"NObeyesdad"</em>, qui cat√©gorise 7 niveaux d‚Äôob√©sit√©.<br><br>
    <strong>Objectifs :</strong>
    <ul>
      <li>Pr√©dire les niveaux d‚Äôob√©sit√© √† l‚Äôaide de mod√®les de machine learning.</li>
      <li>Identifier les facteurs influents pour fournir des recommandations de sant√© publique adapt√©es √† chaque pays.</li>
    </ul>
  </p>
  
  <p><strong>T√¢ches effectu√©es :</strong></p>
  <ol>
    <li><strong>Compr√©hension & Pr√©paration des Donn√©es</strong>
      <ul>
        <li>Analyse des variables qualitatives et quantitatives</li>
        <li>Ajout de l‚ÄôIMC (poids / taille¬≤)</li>
        <li>D√©tection des valeurs manquantes et aberrantes</li>
        <li>Recodage des variables num√©riques d√©cimales (FCVC, NCP, CH2O, FAF, TUE)</li>
        <li>Encodage des variables cat√©gorielles avec <code>LabelEncoder()</code></li>
      </ul>
    </li>
    <li><strong>Nettoyage et Structuration</strong>
      <ul>
        <li>Suppression de variables non pertinentes : <em>TUE, CALC, CH2O, SMOKE, Gender</em></li>
        <li>Cr√©ation de plusieurs DataFrames : <em>obese, obesecluster, obesecluster_array, attributs_data, target_data</em></li>
      </ul>
    </li>
    <li><strong>Analyse exploratoire</strong>
      <ul>
        <li>Matrice de corr√©lation pour √©tudier les relations entre les variables</li>
        <li>Identification des variables influentes dans la pr√©diction de l‚Äôob√©sit√©</li>
      </ul>
    </li>
    <li><strong>Apprentissage non supervis√©</strong>
      <ul>
        <li>Clustering K-Means avec 7 puis 5 clusters</li>
        <li>R√©duction de dimension (PCA) pour visualisation 2D</li>
      </ul>
    </li>
    <li><strong>Apprentissage supervis√©</strong>
      <p>Mod√®les utilis√©s :</p>
      <ul>
        <li>DecisionTreeClassifier (Accuracy ‚âà 96%)</li>
        <li>DecisionTreeRegressor (‚âà 96.5%)</li>
        <li>Gaussian Naive Bayes (‚âà 83.7%)</li>
        <li>R√©gression Logistique (‚âà 96.7%)</li>
        <li>Support Vector Machine (SVM) (‚âà 96.06%)</li>
        <li>K-Nearest Neighbors (KNN) (‚âà 92.27%)</li>
      </ul>
      <p>√âvaluation des mod√®les :</p>
      <ul>
        <li>Accuracy, Pr√©cision, Rappel, F1-Score</li>
        <li>Matrices de confusion, courbes ROC</li>
        <li>Erreurs : absolue, quadratique (EQ), EQM</li>
      </ul>
    </li>
    <li><strong>S√©lection du mod√®le optimal</strong>
      <ul>
        <li>Mod√®les les plus performants : R√©gression logistique, DecisionTreeRegressor, SVM</li>
      </ul>
    </li>
    <li><strong>R√®gles de classification interpr√©tables</strong>
      <ul>
        <li>Cr√©ation de r√®gles simples bas√©es sur l‚ÄôIMC, la taille, le poids, les ant√©c√©dents familiaux, etc.</li>
      </ul>
    </li>
    <li><strong>√âvaluation financi√®re (√©tude compl√©mentaire)</strong>
      <ul>
        <li>Analyse co√ªt-b√©n√©fice des politiques publiques de pr√©vention</li>
        <li>√âvaluation du retour sur investissement (ROI), b√©n√©fices √©conomiques et sociaux</li>
      </ul>
    </li>
  </ol>

  <p><strong>Outils :</strong></p>
  <ul>
    <li><strong>Langage :</strong> Python</li>
    <li><strong>Biblioth√®ques principales :</strong>
      <table border="1" cellpadding="5" cellspacing="0">
        <thead>
          <tr>
            <th>Biblioth√®que</th>
            <th>Utilit√© principale</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>pandas</code></td>
            <td>Manipulation et structuration des donn√©es</td>
          </tr>
          <tr>
            <td><code>numpy</code></td>
            <td>Calculs num√©riques</td>
          </tr>
          <tr>
            <td><code>matplotlib</code>, <code>seaborn</code></td>
            <td>Visualisation de donn√©es et des courbes de ROC</td>
          </tr>
          <tr>
            <td><code>scikit-learn (sklearn)</code></td>
            <td>Mod√©lisation : DecisionTree, SVM, kNN, LogisticRegression, etc.</td>
          </tr>
          <tr>
            <td><code>LabelEncoder</code></td>
            <td>Encodage des variables cat√©gorielles</td>
          </tr>
          <tr>
            <td><code>KMeans</code>, <code>PCA</code></td>
            <td>Clustering et r√©duction de dimensions</td>
          </tr>
        </tbody>
      </table>
    </li>
  </ul>
</section>

     <section class="project">
  <h2>D√©tection de fake news via Deep Learning</h2>
  <p><strong>Contexte :</strong> Lutte contre la d√©sinformation diffus√©e sur les r√©seaux sociaux, notamment concernant la sant√© des dirigeants.</p>

  <p><strong>T√¢ches effectu√©es :</strong> Collecte de donn√©es (Facebook, Twitter, etc.), traitement en CSV, NLP, entra√Ænement de mod√®les pour la d√©tection automatique de fausses nouvelles.</p>

  <p>Pour ce projet, j‚Äôai choisi de m‚Äôappuyer sur l‚Äôintelligence artificielle (IA), qui offre une solution prometteuse pour automatiser la d√©tection des fake news. En utilisant des techniques avanc√©es de traitement du langage naturel (NLP), j‚Äôai r√©colt√© des donn√©es √† partir de diverses sources telles que Facebook, Google, Twitter, la presse et YouTube.</p>

  <p>Par exemple, j‚Äôai r√©cup√©r√© toutes les vid√©os √©voquant les fausses nouvelles sur la sant√© du pr√©sident. Je me suis concentr√© sur les commentaires trouv√©s sur Google, j‚Äôai collect√© des tweets li√©s √† ce sujet sur Twitter, rassembl√© des articles de presse, et analys√© les publications et commentaires sur Facebook.</p>

  <p>J‚Äôai ensuite structur√© mes donn√©es dans un fichier CSV comportant quatre colonnes :
    <ul>
      <li><strong>sources :</strong> origine des donn√©es (site web ou plateforme sociale),</li>
      <li><strong>commentaire :</strong> texte de la nouvelle ou du commentaire,</li>
      <li><strong>malade :</strong> identification du sujet (ici, la sant√© du pr√©sident),</li>
      <li><strong>auteur :</strong> nom ou identifiant de l‚Äôauteur du contenu.</li>
    </ul>
  </p>

  <p>Apr√®s avoir pr√©par√© ces fichiers CSV, j‚Äôai utilis√© la biblioth√®que Pandas en Python pour les concat√©ner en un seul fichier. Gr√¢ce √† la fonction <code>concat()</code>, j‚Äôai pu fusionner toutes les donn√©es en un fichier consolid√© pr√™t √† l‚Äôanalyse.</p>

  <p>Ce fichier unique contient l‚Äôensemble des donn√©es structur√©es. La colonne "sources" permet de retracer l‚Äôorigine de chaque entr√©e, "commentaire" fournit le contenu textuel, "malade" permet d‚Äôidentifier les fausses nouvelles li√©es √† la sant√©, et "auteur" indique l‚Äôorigine du contenu.</p>

  <p>Ce processus de collecte et de structuration des donn√©es est essentiel pour entra√Æner un syst√®me d‚ÄôIA performant. Il facilite l‚Äôanalyse des tendances, la d√©tection des motifs r√©currents dans les fausses nouvelles, et l‚Äôidentification de sources peu fiables.</p>

  <p>En r√©sum√©, j‚Äôai pris en charge l‚Äôint√©gralit√© de la collecte et de l‚Äôorganisation des donn√©es, cr√©ant ainsi une base robuste pour mon projet de d√©tection de fake news.</p>

  <p><strong>Outils :</strong> Python, Pandas, NumPy, Scikit-learn, TensorFlow, Matplotlib, Seaborn, CountVectorizer, TfidfVectorizer, Stopwords, Logistic Regression, Random Forest, SVM, <code>accuracy_score</code>, <code>f1_score</code>, <code>confusion_matrix</code>.</p>

</section>




  <section class="project">
  <h2>Assistant IA ‚Äì Jeux Olympiques 2024</h2>

  <p><strong>Contexte :</strong>  
  D√©veloppement d‚Äôun chatbot pr√©dictif bas√© sur l‚ÄôIA pour r√©pondre √† toutes les questions li√©es aux Jeux Olympiques de Paris 2024, notamment les classements, les performances des athl√®tes, et les pr√©visions de m√©dailles.</p>

  <p><strong>T√¢ches effectu√©es :</strong>  
  J‚Äôai con√ßu et test√© plusieurs prompts sp√©cialis√©s afin d‚Äôadapter les r√©ponses du chatbot selon le contexte :  
  <ul>
    <li><code>JO_PROMPT</code> : Fournit les informations officielles sur le classement des JO 2024.</li>
    <li><code>JO_PROMPT2</code> : Pr√©sente les performances remarquables et les m√©daill√©s marquants des JO.</li>
    <li><code>JO_PROMPT3</code> : Prompt destin√© √† un assistant expert en sport. Il analyse les chances de m√©dailles d‚Äôun pays ou d‚Äôun athl√®te en se basant sur les r√©sultats pr√©c√©dents, les classements mondiaux, et les tendances observ√©es, sans jamais donner de certitudes.</li>
  </ul>
  Ce syst√®me de prompts permet une r√©ponse contextuelle, fiable et nuanc√©e selon le type de requ√™te utilisateur.</p>

  <p><strong>Outils :</strong> Python, OpenAI API, Streamlit</p>
</section>

  <section class="project">
  <h2>D√©veloppement d'une application carbone</h2>
  <p>
    <strong>Contexte :</strong> D√©veloppement d‚Äôun outil innovant dans Revit pour estimer l‚Äôempreinte carbone des projets architecturaux d√®s les phases d‚Äô√©tudes pr√©liminaires et d‚Äôavant-projet.  
    Cet outil permet de calculer une estimation globale du bilan carbone √† partir des donn√©es disponibles dans le mod√®le Revit.  
    Il aide les d√©cideurs √† comprendre rapidement l‚Äôimpact environnemental potentiel de leurs constructions et √† prendre des d√©cisions plus durables.
  </p>

  <p>
    L‚Äôoutil a √©t√© con√ßu pour √™tre facilement int√©grable et simple d‚Äôutilisation. Il permet √©galement de g√©n√©rer des rapports automatis√©s ainsi que des vues 3D color√©es pour visualiser l‚Äôimpact carbone de chaque composant du b√¢timent.  
    j'ai  jou√© un r√¥le cl√© dans son d√©veloppement, notamment dans la programmation en Python et l‚Äôint√©gration technique avec Revit.  
    Gr√¢ce √† ma contribution, l‚Äôoutil sera d√©ploy√© chez AREP √† l‚Äô√©t√© 2025.
  </p>

  <p><strong>T√¢ches effectu√©es :</strong> D√©veloppement d‚Äôune application sur Revit (Python/C#), g√©n√©ration automatique du calcul d‚Äôempreinte carbone, visualisation interactive avec Dash & Power BI, assistance √† la prise de d√©cision √©cologique.</p>

  <p><strong>Outils :</strong> Python, Revit, C#, Dash, Power BI, Git, GitHub.</p>

</section>



  </main>
  <footer>
    <p>Souleymane Daffe  DATA SCIENTIS/ANALYST/DEV IA</p>
  </footer>
</body>
</html>
